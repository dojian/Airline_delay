{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34cf8b09-83f1-4ac6-9304-56a1ce50ce5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorSlicer,VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f868673-ed6f-4e7d-95d2-6a91e089b8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blob_container  = \"w261projectteam33\"       # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"w261projectteam33\"  # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope    = \"w261projectteam33_scope\"          # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key      = \"w261projectteam33\"             # The name of the secret key created in your local computer using the Databricks CLI\n",
    "\n",
    "team_blob_url   = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"  #points to the root of your team storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad1e30c-ec3b-448f-91ab-657169a488c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# the 261 course blob storage is mounted here.\n",
    "mids261_mount_path = \"/mnt/mids-w261\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2724251b-5fc6-4557-a2a1-d21e4a6d8cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "\n",
    "#df_cleaned_row.write.parquet(f\"{team_blob_url}/1yr_otpw_clean\")\n",
    "\n",
    "# see what's in the blob storage root folder \n",
    "#display(dbutils.fs.ls(f\"{team_blob_url}\"))\n",
    "#        \"test\": f\"{team_blob_url}/5yr_step7_holdout_pagerank_df\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b1d426-06b8-48df-bd42-40ac9b37473f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Cross Validation data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9e5f75-94cf-4fac-8fd5-def26854cd24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the paths for the cross-validation datasets 1yr_otpw_downsample_train_{fold}\n",
    "cv_data_paths = {\n",
    "    \"fold_0\": {\"train\": f\"{team_blob_url}/cross_validation/5yr_step8_downsampled_no_pr_train_fold_fold_0\",\n",
    "               \"validation\": f\"{team_blob_url}/cross_validation/5yr_step6_cv_validation_fold_0\"},\n",
    "    \"fold_1\": {\"train\": f\"{team_blob_url}/cross_validation/5yr_step8_downsampled_no_pr_train_fold_fold_1\",\n",
    "               \"validation\": f\"{team_blob_url}/cross_validation/5yr_step6_cv_validation_fold_1\"},\n",
    "    \"fold_2\": {\"train\": f\"{team_blob_url}/cross_validation/5yr_step8_downsampled_no_pr_train_fold_fold_2\",\n",
    "               \"validation\": f\"{team_blob_url}/cross_validation/5yr_step6_cv_validation_fold_2\"},\n",
    "    \"fold_3\": {\"train\": f\"{team_blob_url}/cross_validation/5yr_step8_downsampled_no_pr_train_fold_fold_3\",\n",
    "               \"validation\": f\"{team_blob_url}/cross_validation/5yr_step6_cv_validation_fold_3\"},\n",
    "    \"fold_4\": {\"train\": f\"{team_blob_url}/cross_validation/5yr_step8_downsampled_no_pr_train_fold_fold_4\",\n",
    "               \"validation\": f\"{team_blob_url}/cross_validation/5yr_step6_cv_validation_fold_4\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51265b6-298e-4b4f-982c-0dca05a2169f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define function to calculate precision and recall\n",
    "def calculate_precision_recall(predictions):\n",
    "    tp = predictions.filter((col(\"final_prediction\") == 1) & (col(\"DEP_DEL15\") == 1)).count()\n",
    "    fp = predictions.filter((col(\"final_prediction\") == 1) & (col(\"DEP_DEL15\") == 0)).count()\n",
    "    fn = predictions.filter((col(\"final_prediction\") == 0) & (col(\"DEP_DEL15\") == 1)).count()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Custom F-beta score function\n",
    "def fbeta_score(predictions, beta=2.0):\n",
    "    tp = predictions.filter(col(\"final_prediction\") == 1).filter(col(\"DEP_DEL15\") == 1).count()\n",
    "    fp = predictions.filter(col(\"final_prediction\") == 1).filter(col(\"DEP_DEL15\") == 0).count()\n",
    "    fn = predictions.filter(col(\"final_prediction\") == 0).filter(col(\"DEP_DEL15\") == 1).count()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (beta**2 * precision + recall) > 0 else 0\n",
    "    return fbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d07a2c-8dc7-49d7-9eb6-3652370e190d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Feature columns\n",
    "numeric_columns = ['DEP_DELAY_lag_1','CRS_ELAPSED_TIME', 'dest_airport_lat','dest_airport_lon', 'origin_airport_lat','origin_airport_lon','ELEVATION','DISTANCE']\n",
    "categorical_columns = ['DAY_OF_WEEK','DAY_OF_MONTH','QUARTER', 'TAIL_NUM','OP_CARRIER_FL_NUM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9335a5-b74e-46d4-897a-b28dee1ceb7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#experimentation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe873c0-a58b-400a-8181-057201b840e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the initial pipeline for preprocessing and XGBoost\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"skip\") for col in categorical_columns]\n",
    "\n",
    "categorical_assembler = VectorAssembler(inputCols=[col + \"_index\" for col in categorical_columns], outputCol=\"categorical_features\")\n",
    "\n",
    "# Combine All Features\n",
    "final_assembler = VectorAssembler(inputCols=[\"categorical_features\"] + numeric_columns, outputCol=\"features\")\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = SparkXGBClassifier(\n",
    "    features_col=\"scaled_features\",  \n",
    "    label_col=\"DEP_DEL15\",\n",
    "    prediction_col=\"xgb_prediction\",  \n",
    "    probability_col=\"xgb_probability\",  \n",
    "    raw_prediction_col=\"xgb_rawPrediction\",  \n",
    "    num_workers=3,  \n",
    "    tree_method='hist',   \n",
    "    max_depth=5,            \n",
    "    eta=0.2,              \n",
    "    num_round=100,          \n",
    "    seed=101,                \n",
    "    missing=0.0,           \n",
    "    enable_sparse_data_optim=True  \n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\", \n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"rf_prediction\",  \n",
    "    probabilityCol=\"rf_probability\",  \n",
    "    rawPredictionCol=\"rf_rawPrediction\",  \n",
    "    numTrees=3, maxDepth=3, seed=123, maxBins=10000\n",
    ")\n",
    "\n",
    "# Assemble features for stacking\n",
    "assembler_stacked = VectorAssembler(\n",
    "    inputCols=[\"xgb_prob_class_1\", \"rf_prob_class_1\"],\n",
    "    outputCol=\"stacked_features\"\n",
    ")\n",
    "\n",
    "# Meta model (Logistic Regression)\n",
    "meta_model = LogisticRegression(\n",
    "    featuresCol=\"stacked_features\", \n",
    "    labelCol=\"DEP_DEL15\", \n",
    "    predictionCol=\"final_prediction\"  \n",
    ")\n",
    "\n",
    "# Define the preprocessing pipeline (without models)\n",
    "preprocessing_pipeline = Pipeline(stages=indexers + [categorical_assembler, final_assembler, scaler])\n",
    "\n",
    "# Define the final pipeline for stacking and meta-model\n",
    "pipeline_stage_2 = Pipeline(stages=[assembler_stacked, meta_model])\n",
    "\n",
    "# Evaluator setup\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", predictionCol=\"final_prediction\")\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7145cc14-5882-43e6-9211-7f1d32506f82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold fold_0...\nLoaded train and validation datasets\nConverted string columns to numeric types\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9891378d224e1ea98ff62a951288fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e21469cd33f44998a82414c742c2dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed the data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:25:26,684 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 3 workers with\n\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'tree_method': 'hist', 'eta': 0.2, 'num_round': 100, 'seed': 101, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': 0.0}\n2024-08-10 23:26:10,444 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran XGBoost on the preprocessed data\nExtracted XGBoost class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f953e58299457088c78b253b485158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca53386e13c481da6c8af73a558cd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RandomForest on the preprocessed data\nExtracted RandomForest class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f460ea31707407eb04965c08ae574e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e3593d927b445ab31024d340d0a1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed the final pipeline\nFold fold_0 results:\n  F2 Score: 0.4062\n  Precision: 0.3713\n  Recall: 0.4159\n\n\nStarting fold fold_1...\nLoaded train and validation datasets\nConverted string columns to numeric types\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5512194cb0e489e88eff6d12e472938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5389d7b38849a4bd87bfb6c39dc4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed the data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:42:27,057 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 3 workers with\n\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'tree_method': 'hist', 'eta': 0.2, 'num_round': 100, 'seed': 101, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': 0.0}\n2024-08-10 23:43:05,058 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran XGBoost on the preprocessed data\nExtracted XGBoost class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39376f6cf4774f5d8861cbcc9de1d39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d7b45fb795407a906db43e95043ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RandomForest on the preprocessed data\nExtracted RandomForest class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01077d14de7044b89482e3d288fa8d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008ef70224104ad0a5c15a4878493ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed the final pipeline\nFold fold_1 results:\n  F2 Score: 0.5188\n  Precision: 0.2683\n  Recall: 0.6767\n\n\nStarting fold fold_2...\nLoaded train and validation datasets\nConverted string columns to numeric types\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ebe1c4bb914039ab36923a700ccfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a55ac6491a481b9095b50ce24bdf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed the data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:58:14,362 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 3 workers with\n\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'tree_method': 'hist', 'eta': 0.2, 'num_round': 100, 'seed': 101, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': 0.0}\n2024-08-10 23:58:46,308 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran XGBoost on the preprocessed data\nExtracted XGBoost class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbdb83eea58437db41fa76fd723bf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a5822b07049a2a0994b4f9eb72ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RandomForest on the preprocessed data\nExtracted RandomForest class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c023c6e01db34b60ae62e483c096e1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57cc2b6fc4f4694bab0a51ec77004b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed the final pipeline\nFold fold_2 results:\n  F2 Score: 0.5745\n  Precision: 0.3992\n  Recall: 0.6453\n\n\nStarting fold fold_3...\nLoaded train and validation datasets\nConverted string columns to numeric types\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61da22e72214fe2bec8a44a2ef280fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0371d41ac6346e6947d25d25f9078cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed the data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:14:30,754 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 3 workers with\n\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'tree_method': 'hist', 'eta': 0.2, 'num_round': 100, 'seed': 101, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': 0.0}\n2024-08-11 00:15:11,276 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran XGBoost on the preprocessed data\nExtracted XGBoost class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf956fe850240f9bb5fdc1d1fe7acb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a03f64e1b643389f5104ec09f700f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RandomForest on the preprocessed data\nExtracted RandomForest class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605c42a0767405ab35bb778c9788acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e348005ff14f0ab45e5dc98f97860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed the final pipeline\nFold fold_3 results:\n  F2 Score: 0.5616\n  Precision: 0.3066\n  Recall: 0.7091\n\n\nStarting fold fold_4...\nLoaded train and validation datasets\nConverted string columns to numeric types\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27985781ed5f4e4b815aa3c27929ff3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aa9ff9e6db403da11ff512854c915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed the data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:28:45,482 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 3 workers with\n\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'tree_method': 'hist', 'eta': 0.2, 'num_round': 100, 'seed': 101, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': 0.0}\n2024-08-11 00:29:19,969 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran XGBoost on the preprocessed data\nExtracted XGBoost class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab312fd3dc6849cda3ed95a7dec636e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be9680e285d484ea1358715be086d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RandomForest on the preprocessed data\nExtracted RandomForest class 1 probabilities\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671525046aec44cdb69d1b8de8192683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92262e37ebad45ad840d028084279fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted and transformed the final pipeline\nFold fold_4 results:\n  F2 Score: 0.5243\n  Precision: 0.3811\n  Recall: 0.5786\n\n\nCross-validation complete.\nFold fold_0 - F2 Score: 0.4062, Precision: 0.3713, Recall: 0.4159\nFold fold_1 - F2 Score: 0.5188, Precision: 0.2683, Recall: 0.6767\nFold fold_2 - F2 Score: 0.5745, Precision: 0.3992, Recall: 0.6453\nFold fold_3 - F2 Score: 0.5616, Precision: 0.3066, Recall: 0.7091\nFold fold_4 - F2 Score: 0.5243, Precision: 0.3811, Recall: 0.5786\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation loop\n",
    "for fold, paths in cv_data_paths.items():\n",
    "    print(f\"Starting fold {fold}...\")\n",
    "\n",
    "    # Load train and validation datasets for this fold\n",
    "    train_df = spark.read.parquet(paths['train'])\n",
    "    validation_df = spark.read.parquet(paths['validation'])\n",
    "    print(\"Loaded train and validation datasets\")\n",
    "\n",
    "    # Convert string columns to numeric types\n",
    "    for col_name in numeric_columns:\n",
    "        train_df = train_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "        validation_df = validation_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    print(\"Converted string columns to numeric types\")\n",
    "\n",
    "    # Preprocess the data (apply transformations except model predictions)\n",
    "    pipeline_model_stage_1 = preprocessing_pipeline.fit(train_df)\n",
    "    train_df_stage_1 = pipeline_model_stage_1.transform(train_df)\n",
    "    validation_df_stage_1 = pipeline_model_stage_1.transform(validation_df)\n",
    "    print(\"Preprocessed the data\")\n",
    "\n",
    "    # Run the XGBoost model on the preprocessed data\n",
    "    xgb_model = xgb_classifier.fit(train_df_stage_1)\n",
    "    train_df_stage_1 = xgb_model.transform(train_df_stage_1)\n",
    "    validation_df_stage_1 = xgb_model.transform(validation_df_stage_1)\n",
    "    print(\"Ran XGBoost on the preprocessed data\")\n",
    "\n",
    "    # Extract the class 1 probability from xgb_probability using vector_to_array\n",
    "    train_df_stage_1 = train_df_stage_1.withColumn(\"xgb_prob_class_1\", vector_to_array(col(\"xgb_probability\"))[1])\n",
    "    validation_df_stage_1 = validation_df_stage_1.withColumn(\"xgb_prob_class_1\", vector_to_array(col(\"xgb_probability\"))[1])\n",
    "    print(\"Extracted XGBoost class 1 probabilities\")\n",
    "\n",
    "    # Run the RandomForestClassifier separately on the preprocessed data (not the XGBoost output)\n",
    "    rf_model = rf_classifier.fit(train_df_stage_1)\n",
    "    train_df_stage_1 = rf_model.transform(train_df_stage_1)\n",
    "    validation_df_stage_1 = rf_model.transform(validation_df_stage_1)\n",
    "    print(\"Ran RandomForest on the preprocessed data\")\n",
    "\n",
    "    # Extract the class 1 probability from rf_probability using vector_to_array\n",
    "    train_df_stage_1 = train_df_stage_1.withColumn(\"rf_prob_class_1\", vector_to_array(col(\"rf_probability\"))[1])\n",
    "    validation_df_stage_1 = validation_df_stage_1.withColumn(\"rf_prob_class_1\", vector_to_array(col(\"rf_probability\"))[1])\n",
    "    print(\"Extracted RandomForest class 1 probabilities\")\n",
    "\n",
    "    # Stage 2: Fit and transform the final pipeline on the same DataFrame (containing both probabilities)\n",
    "    pipeline_model_stage_2 = pipeline_stage_2.fit(train_df_stage_1)\n",
    "    train_df_final = pipeline_model_stage_2.transform(train_df_stage_1)\n",
    "    validation_df_final = pipeline_model_stage_2.transform(validation_df_stage_1)\n",
    "    print(\"Fitted and transformed the final pipeline\")\n",
    "\n",
    "    # Calculate precision and recall using the custom function\n",
    "    precision, recall = calculate_precision_recall(validation_df_final)\n",
    "    \n",
    "    # Calculate F2 score using the custom function\n",
    "    f2_score = fbeta_score(validation_df_final, beta=2.0)\n",
    "\n",
    "    # Store the metrics\n",
    "    results.append({\n",
    "        \"fold\": fold,\n",
    "        \"f2_score\": f2_score,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    })\n",
    "\n",
    "    # Print the results for the current fold\n",
    "    print(f\"Fold {fold} results:\")\n",
    "    print(f\"  F2 Score: {f2_score:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# After all folds are completed, print the summary of results\n",
    "print(\"Cross-validation complete.\")\n",
    "for result in results:\n",
    "    print(f\"Fold {result['fold']} - F2 Score: {result['f2_score']:.4f}, Precision: {result['precision']:.4f}, Recall: {result['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560507f2-18e9-4d11-953e-2446a5a0d8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated precision, recall, and F2 score\n"
     ]
    }
   ],
   "source": [
    " # Calculate precision and recall using the custom function\n",
    "precision, recall = calculate_precision_recall(validation_df_final)\n",
    "    \n",
    "# Calculate F2 score using the custom function\n",
    "f2_score = fbeta_score(validation_df_final, beta=2.0)\n",
    "print(\"Calculated precision{}, recall, and F2 score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c092a06b-33e1-4892-b15a-a0a29531f583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.37130790203096575"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f809036-bcbe-4081-8ec5-c9c9f3f7bdf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4159467425551377"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c2726d-1371-4f2c-96ae-4407999b0bbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4061804940067907"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fe95fa-cbba-43bf-8151-74e5c2e07e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test dataset\nConverted string columns to numeric types in test set\nPreprocessed the test data\nApplied XGBoost to the test data\nExtracted XGBoost class 1 probabilities for the test data\nApplied RandomForest to the test data\nExtracted RandomForest class 1 probabilities for the test data\nApplied the final pipeline to the test data\nTest Set Results:\n  F2 Score: 0.5505\n  Precision: 0.3732\n  Recall: 0.6247\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_path = f\"{team_blob_url}/5yr_step5_holdout_df\"\n",
    "test_df = spark.read.parquet(test_path)\n",
    "print(\"Loaded test dataset\")\n",
    "\n",
    "# Convert string columns to numeric types in test set\n",
    "for col_name in numeric_columns:\n",
    "    test_df = test_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "print(\"Converted string columns to numeric types in test set\")\n",
    "\n",
    "# Preprocess the test data using the trained preprocessing pipeline\n",
    "test_df_stage_1 = pipeline_model_stage_1.transform(test_df)\n",
    "print(\"Preprocessed the test data\")\n",
    "\n",
    "# Apply the XGBoost model to the preprocessed test data\n",
    "test_df_stage_1 = xgb_model.transform(test_df_stage_1)\n",
    "print(\"Applied XGBoost to the test data\")\n",
    "\n",
    "# Extract the class 1 probability from xgb_probability using vector_to_array\n",
    "test_df_stage_1 = test_df_stage_1.withColumn(\"xgb_prob_class_1\", vector_to_array(col(\"xgb_probability\"))[1])\n",
    "print(\"Extracted XGBoost class 1 probabilities for the test data\")\n",
    "\n",
    "# Apply the RandomForest model to the preprocessed test data\n",
    "test_df_stage_1 = rf_model.transform(test_df_stage_1)\n",
    "print(\"Applied RandomForest to the test data\")\n",
    "\n",
    "# Extract the class 1 probability from rf_probability using vector_to_array\n",
    "test_df_stage_1 = test_df_stage_1.withColumn(\"rf_prob_class_1\", vector_to_array(col(\"rf_probability\"))[1])\n",
    "print(\"Extracted RandomForest class 1 probabilities for the test data\")\n",
    "\n",
    "# Apply the final meta-model to the test data\n",
    "test_df_final = pipeline_model_stage_2.transform(test_df_stage_1)\n",
    "print(\"Applied the final pipeline to the test data\")\n",
    "\n",
    "# Calculate precision and recall using the custom function\n",
    "precision, recall = calculate_precision_recall(test_df_final)\n",
    "\n",
    "# Calculate F2 score using the custom function\n",
    "f2_score = fbeta_score(test_df_final, beta=2.0)\n",
    "\n",
    "# Print the evaluation metrics for the test set\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"  F2 Score: {f2_score:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec3cba7-2879-411c-88ad-5cc62cfa191c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(debug) (Non-Pagerank) Ensemble_5yr_P3_Dong_otpw_model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
